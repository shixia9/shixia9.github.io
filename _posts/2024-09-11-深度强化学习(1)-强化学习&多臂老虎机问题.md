---
math: true
categories: [DRL, Theory]
tags: [deep reinforcement learning]
---

![image-20240830101534119](/common/images/image-20240830101534119.png)

**强化学习**（reinforcement learning, RL）讨论的是智能体（agent）如何在复杂、不确定的环境（environment）中最大化它能够获得的奖励。

强化学习与监督学习：

+ 监督学习输入的数据（标注的数据）都是**没有关联**的；

+ 在监督学习中，我们会告诉模型**正确的标签**是什么，以此模型能够根据正确的标签通过反向传播来修正自己的预测；

强化学习并不满足以上两点

## 相关概念
+ 状态（State）：智能体对环境的观察；
+ 动作（Action）：智能体与环境的交互；
+ 奖励（Reward）：智能体在对应的环境下和给定的时间步长t中执行操作后获得的加成；
  
## 强化学习智能体的类型
通过智能体**有没有学习环境模型**来对智能体分类：

+ 有模型（model-based）强化学习智能体通过学习状态的转移来采取动作。

+ 免模型（model-free）强化学习智能体没有去直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数和策略函数进行决策。

## 多臂老虎机问题
> 在多臂老虎机（multi-armed bandit，MAB）问题中，有一个拥有 $$ K $$ 根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布 $$ R $$ 。我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励 $$ r $$ 。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作 $$ T $$ 次拉杆后获得尽可能高的累积奖励。由于奖励的概率分布是未知的，因此我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆“中进行权衡。“**采用怎样的操作策略才能使获得的累积奖励最高**”便是多臂老虎机问题。

### 数学建模

多臂老虎机问题可以表示为一个元组 $$ \langle A, R \rangle $$ ，其中：

+ \$$ A $$ 为动作集合，表示一个动作拉动一个拉杆。则动作空间集合为 \$$ \{a_1, ..., a_K\} $$ ，其中 \$$ a_t \in A $$ 表示任意一个动作；

+ \$$ R $$ 为奖励概率分布，拉动每一个拉杆的动作 \$$ a $$ 都对应一个奖励概率分布 \$$ R(r, a) $$ ，不同拉杆的奖励分布通常是不同的。

假设每个时间步长中间拉动一个拉杆，多臂老虎机的目标为最大化一段时间步长 $$ T $$ 内累计的奖励： $$ \max \sum_{t=1}^Tr_t, r_t\sim R(\cdot|a_t) $$ 。其中 $$ a_t $$ 表示在第 $$ t $$ 时间步内拉动某一拉杆的动作， $$ r_t $$ 表示动作 $$ a_t $$ 获得的奖励。

### 累积懊悔

对于每个动作 $$ a $$ ，定义其期望奖励为： $$ Q(a)=\Bbb{E}_r\sim R(\cdot|a)[r] $$，因此存在最优期望奖励为： $$ Q^*=\max_{a\in A}Q(a) $$ 。

为了更方便观察每次拉动拉杆的期望奖励距离最有期望奖励的差距，引入了**懊悔**（regret）的概念，即拉动当前拉杆的动作 $$ a $$ 与最优拉杆的奖励差： $$ R(a)=Q^*-Q(a) $$ 。**累积懊悔**（cumulative regret）即操作 $$ T $$ 次后累积的总懊悔量： $$ \sigma_R=\sum^T_{t=1}R(a_t) $$ 。

MAB问题的目标为最大化累积奖励，也等价于最小化累积懊悔。

### 估计期望奖励

为了知道拉动哪一个拉杆能获得最高奖励，需要估计拉动这根拉杆的期望奖励。如果只拉动一次这个拉杆获得的奖励存在随机性，所以需要多次拉动一根拉杆，计算多次奖励的期望。

对于 $$ \forall a\in A $$ ，初始化计数器 $$ N(a)=0 $$ 和期望奖励估值 $$ \hat Q(a)=0 $$ 

+  $$ for\space t=1\to T\space do $$ 
+ 选取某个拉杆，该动作为 $$ a_t $$ 
+ 得到奖励 $$ r_t $$ 
+ 更新计数器 $$ N(a_t)=N(a_t)+1 $$ 
+ 更新期望奖励估值： $$ \hat Q(a_t)=\hat Q(a_t)+{1 \over N(a_t)}[r_t-\hat Q(a_t)] $$ 

>**推导**
>$$
>\begin{align}
>\hat Q_k&=\frac 1k\sum_{i=1}^kr_i\\
>&=\frac 1k(r_k+\sum_{i=1}^{k-1}r_i)\\
>&=\frac 1k[r_k+(k-1)\hat Q_{k-1}]\\
>&=\hat Q_{k-1}+\frac 1k(r_k-\hat Q_{k-1})
>\end{align}
>$$

### 代码实现

实现一个多臂老虎机类

```python
class Bandit:
    def __init__(self, K):
        self.prob = np.random.uniform(size=K)  # 随机生成每根拉杆的获奖概率
        self.best_idx = np.argmax(self.prob)  # 获奖概率最大的拉杆
        self.best_prob = self.prob[self.best_idx]  # 最大获奖概率
        self.K = K

    def step(self, k):
        # 拉动拉杆后，返回结果：1获奖，0未获奖
        if self.prob[k] > np.random.rand():
            return 1
        else:
            return 0

np.random.seed(0)
K = 10
bandit_10_arm = Bandit(K)  # 生成一个10臂的老虎机
print(f'获奖概率最大的拉杆为 {bandit_10_arm.best_idx} 号，获奖概率为 {bandit_10_arm.best_prob:.4f}')
```

```
获奖概率最大的拉杆为 8 号，获奖概率为 0.9637
```

多臂老虎机算法基本框架：

```python
class Solver:
    def __init__(self, bandit):
        self.bandit = bandit
        self.counts = np.zeros(self.bandit.K)  # 记录每根拉杆的尝试次数
        self.regret = 0.  # 当前步的累积懊悔
        self.actions = []  # 记录每一步的动作（拉动了哪一根拉杆）
        self.regrets = []  # 记录每一步的累积懊悔

    def update_regret(self, k):
        # 计算拉动k拉杆的累积懊悔
        self.regret += self.bandit.best_prob - self.bandit.probs[k]
        self.regrets.append(self.regret)

    def run_one_step(self):
        # 根据策略选择拉杆，并返回当前动作选择的拉杆编号
        raise NotImplementedError

    def run(self, num_steps):
        # 运行num_steps次
        for _ in range(num_steps):
            k = self.run_one_step()
            self.counts[k] += 1
            self.actions.append(k)
            self.update_regret(k)
```

拉杆的策略选择算法：

+ **epsilon贪婪算法**（ $$ \epsilon $$ -Greedy）：每次以概率 $$ 1-\epsilon $$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $$ \epsilon $$ 随机选择一根拉杆（探索），即：
  $$
  \begin{equation}
    a*t=
    \begin{cases}
    arg\space max\space *a\in A\hat Q(a),&采样概率：1-\epsilon\\
    从A中随机选择,&采样概率：\epsilon
    \end{cases}
  \end{equation}
  $$
  在 $$ \epsilon $$ -Greedy的具体是现在， $$ \epsilon $$ 随时间衰减（但不会降低至0），即探索的概率会不断降低。

```python
class EpsilonGreedy(Solver):
    def __init__(self, bandit, init_prob=1.0):
        super(EpsilonGreedy, self).__init__(bandit)
        self.epsilon = 0
        self.estimates = np.array([init_prob] * self.bandit.K)

    def run_one_step(self):
        self.epsilon += 1
        if np.random.random() < 1 / self.epsilon:  # epsilon随时间衰减
            k = np.random.randint(0, self.bandit.K)  # 随机选择一个拉杆（探索）
        else:
            k = np.argmax(self.estimates)  # 选择期望奖励估值最大的拉杆（利用）
        r = self.bandit.step(k)  # 得到本次动作的奖励
        # 更新期望奖励估计值
        self.estimates[k] += 1. / (self.counts[k] + 1) * (r - self.estimates[k])
        return k


np.random.seed(1)
epsilon_greedy_solver = EpsilonGreedy(bandit_10_arm)
epsilon_greedy_solver.run(5000)
print('epsilon-贪婪算法的累积懊悔为：', epsilon_greedy_solver.regret)
PlotResult([epsilon_greedy_solver], ["EpsilonGreedy"])
```

```
epsilon-贪婪算法的累积懊悔为： 9.425544033015449
```

![image-20240831154516607](/common/images/image-20240831154516607.png)

+ **上置信界算法**（upper confidence bound，UCB）：一种经典的基于不确定性的策略算法，其用到的数学原理为**霍夫丁不等式**（Hoeffding's inequality），即：令 $$ X_1,...,X_n $$ 为 $$ n $$ 个独立同分布的随机变量，取值范围为 $$ [0, 1] $$ ，其经验期望为 $$ \bar x_n=\frac 1n\sum_{j=1}^nX_j $$ ，则有
  $$
  \begin{equation}
    \Bbb{P}\{\Bbb{E}[X]\geq \bar x_n+u\}\leq e^{-2nu^{2}}
  \end{equation}
  $$
  在多臂老虎机问题中，期望奖励估值 $$ \hat Q_t(a) $$ 即为 $$ \bar x_n $$ ，不确定变量 $$ u=\bar U_t(a) $$ ，根据上面的不等式，给定一个概率 $$ p=e^{-2nu^{2}} $$ ，那么 $$ Q_t(a)\lt \hat Q_t(a)+\hat U_t(a) $$ 至少以概率 $$ 1-p $$ 成立。当 $$ p $$ 很小时，那么该不等式就以很大的概率成立，这时 $$ \hat Q_t(a)+\hat U_t(a) $$ 就是期望奖励上界，选取期望奖励最大的动作 $$ a $$ 作为下一步操作。
  
  根据不等式，解出不确定性度量
    $$
    \begin{equation}
        \hat U_t(a) = \sqrt{ \frac{ -\log p }{ 2 N_t(a) } }
    \end{equation}
    $$
  ，因此，设定了一个概率 $$ p $$ 后，就可以计算出不确定性度量 $$ \hat U_t(a) $$ 。

```python
class UCB(Solver):
    def __init__(self, bandit, coef, init_prob=1.0):
        super(UCB, self).__init__(bandit)
        self.t = 0
        self.estimates = np.array([init_prob] * self.bandit.K)
        self.coef = coef  #

    def run_one_step(self):
        self.t += 1
        ucb = self.estimates + self.coef * np.sqrt(
            np.log(self.t) / (2 * (self.counts + 1)))  # 计算上置信界
        k = np.argmax(ucb)  # 选出上置信界最大的拉杆
        r = self.bandit.step(k)
        self.estimates[k] += 1. / (self.counts[k] + 1) * (r - self.estimates[k])
        return k


np.random.seed(1)
coef = 1
UCB_solver = UCB(bandit_10_arm, coef)
UCB_solver.run(5000)
print('上置信界算法的累积懊悔为：', UCB_solver.regret)
PlotResult([UCB_solver], ["UCB"])
```

```
上置信界算法的累积懊悔为： 93.32218503122954
```

![image-20240831164613602](/common/images/image-20240831164613602.png)

+ **汤普森采样**（Thompson sampling）：假设拉动每个拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。汤普森采样根据当前每个动作 $$ a $$ 的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。

```python
class ThompsonSampling(Solver):
    def __init__(self, bandit):
        super(ThompsonSampling, self).__init__(bandit)
        self._one = np.ones(self.bandit.K)  # 每根拉杆奖励为1的次数
        self._zero = np.ones(self.bandit.K)  # 每根拉杆奖励为0的次数

    def run_one_step(self):
        samples = np.random.beta(self._one, self._zero)
        k = np.argmax(samples)
        r = self.bandit.step(k)
        self._one[k] += r
        self._zero[k] += (1 - r)
        return k


np.random.seed(1)
TS_solver = ThompsonSampling(bandit_10_arm)
TS_solver.run(5000)
print('上置信界算法的累积懊悔为：', TS_solver.regret)
PlotResult([TS_solver], ["ThompsonSampling"])
```

```
汤普森采样算法的累积懊悔为： 25.543567041570626
```

![image-20240831170543856](/common/images/image-20240831170543856.png)